### array packed vs. 64-byte spread

Preliminary research: Compiling the program with g and -Og, seeing what
addresses it's accessing. Putting the program into gdb, seeing what addresses
it's accessing. Seems that -i 1 is accessing literally consecutive bytes in memory.

Hypothesis: When accessing a byte of memory, the processor ensures that the 64-byte
cache line that contains that byte is in the L1 cache. Subsequent reads of bytes
in the same cache line only result in pulling the memory from a slower cache if
the L1 cache is full. -i 1 is faster than -i 64 because it does more
time-consecutive actions in the same cache line.

Prediction 1: An interval less than 64 will be faster than -i 64, but more than
-i 1.

Result: True for 25 (3.4 ns/ops), 32 (3.7 ns/ops), and 48 (5.6 ns). For
reference: 64 (7.5 ns/ops)

Prediction: Higher multiples of 64 for the interval will not change the runtime,
until it slows the thing down, and then the increasing the size can speed it
back up.

Result: Completely false. Going up by powers of 2 with the interval, increases
the time it takes more than linearly until it starts flattening out around 512,
then still steadily increases, at least through 4096. Maybe other caches than L1
are relevant? I was hoping they weren't...

Prediction: Increasing the size will increase the time it takes, because the
caches will fill up more times and there will be more cache misses and
evictions.

Result: Impossible to increase size (integer overflow, I assume). If I decrease
size (at least for the neighboring two powers of two), the time doesn't really
change - prediction is false. I kind of thought that their might be an upper
limit on how much filling up the cache matters, and I'm guessing we're beyond
it, but I can't quite put my finger on why. I'm guessing it's because the number
of cache misses stays about the same, as long as the amount of memory allocated
is greater than the size of some cache available to the process (how do caches
get divvied up among processes? is it really first-come-first-serve?)

Prediction: Reducing the size to below 33 MiB (the amount of L3 cache on this
system) is gonna speed it up. That translates to 33554432 bytes (33 MiB).